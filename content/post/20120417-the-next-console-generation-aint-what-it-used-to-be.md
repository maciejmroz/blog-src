+++
title = "The next console generation ain't what it used to be"
date = "2012-04-17"
slug = "the-next-console-generation-aint-what-it-used-to-be"
categories = [ "Game Industry" ]
tags = [ "Apple", "Sony", "Nintendo", "Microsoft"]
+++

The next generation of consoles will most likely come by the end of year 2013. One intersting question arise: how it **should** look like? Disclaimer: by "should" I mean "actually makes business sense for both platform holders and developers" and not "what hardcore gamers would like to see". These meanings are at quite a big conflict, as we shall see later :)

In order to think about next gen we have to take deeper look at the general trends in the entertainment industry. Some of these trends were not significant at X360/PS3 launch, some did not even exist at the time. The world has radically changed in the past few years, invalidating many assumptions entire console industry was built on. As a reminder, the classic console business model is based on selling hardware at a loss in order to build userbase and later getting the money back in the form of licensing fees (which implies absolute control over platform). Traditionally, consoles were very powerful at release - comparable to very good gaming PC. Other than cost optimizations and physical layout, during lifetime of a console hardware specifications are fixed.

This provided value to customers in two ways: one way was getting very powerful hardware at a discounted price, and second was getting stable hardware configuration that is guaranteed to be compatible with every game released for the system. Of course, console gamers eventually end up paying more than PC gamers but because it is spread over long long time, it's something people generally tend to overlook. It's all great - if it works. However, if the console doesn't get enough traction it is a financial disaster. Developing custom hardware and operating system is insanely costly and time consuming. We're not only talking custom boards and software here - we're talking CPU/GPU design! At launch, instead of starting to make money on the console, **even more** money is spent. It's a huge risk to take.

Subsidizing the hardware is something that was already put into doubt at the very beginning of the current console cycle by Nintendo. Wii was never sold at a loss, allowing the profit to be made on both games and hardware itself. It is reasonable to assume that designing the Wii was a lot cheaper than designing PS3/X360 - essentially is was slightly beefed up Gamecube with a new controller. Obviously, the break even point for Nintendo was much lower than for Sony/Microsoft, something I am sure both companies noticed. What everyone noticed is that there's huge group of people who didn't mind worse graphics quality, and didn't care about HD that much. I am not trying to convince anyone here that better graphics and all other stuff brought by high performance hardware do not matter. They do. But with every hardware generation the group of people who care is actually smaller and smaller.

Now some personal observation. While I do not play games on a PC that much, I managed to notice one thing related to console cycles. Every time game console is released PC players say "wow!" (deep inside, of course, no PC gamer will openly admit that :) ). Shortly (a year or so later) PC hardware and games are roughly the same quality. Fast forward two years and PC games are vastly superior (this is the moment when PC gamers proudly bash console gamers in public :) ). Finally, when new console generation arrives, everyone says "it's about time". I don't see this cycle happening right now ... sure, there are some DX11 titles that look very good (and look at tech demos from companies like Epic and Crytek!) but I don't see people agonizing over the need for new GPU, or even complete system. Also, I don't see people getting excited about new chips from AMD/Nvidia or new Intel CPUs. Hardcore gamers might still be in denial, but I'll say it anyway: graphics quality is at the stage where providing even small gains in user experience requires expotential growth in computing power.

In order to make people say "wow!" again next gen systems would need **a lot**. How much? For starters, we need to double GPU power just to port existing content to 1080p. I'd say that for a meaningful boost in quality, we'd also need 4x programmable shading performance on top of that - so 8x the GPU performance as a bare minimum. It's safe to assume we'd also need a lot more memory - just boosting the texture resolution 2x requires 4x the memory. But we'd want to have HDR rendering pipeline. Ooops, that's bigger framebuffer, and probably some HDR textures. Not to mention a lot of modern rendering techniques require floating point buffers and textures anyway. Add more geometry storage, too (even if in the form of displacement maps). So, going "the old way" next gen console would need at least 4GB of total memory and 2TFLOPS of programmable GPU, all that paired with significantly faster CPU. This is doable even today, but even if you ignore costs, there is another catch in here, and that's ... power consumption. The correct way of thinking about CPU/GPU today is only one: performance/Watt. Fitting above spec within 200W envelope is going to be very challenging, even with 22nm CPU/GPU process. Can Sony/MS take a risk and go with a lot more powerful hardware at the expense of noise and power usage? I don't think so. But that's exactly what they would have to do make games significantly better looking.

Now look for a while at the development side of things. Some game budgets are already in line with Hollywood movies. Next gen as described above would require a lot more content, and new content production pipelines. Insane budgets would only get bigger ... There's room for very few blockbusters out there, and even for them the business is getting tougher and tougher. Money and effort is likely to naturally flow in the direction of lower risk and higher ROI, and might mean moving away from traditional consoles. It's not like developers don't have a choice. In fact, right now developers have more choice than ever! First of all, there's iOS and Android, in both smartphone and tablet form factors. These platforms have relatively low barriers to entry and and are natural target for small/medium development teams. There's also Steam on PC, along with some smaller online stores. Last but not least, there's Facebook, along with smaller social networks. Any of these are (from developer point of view) better platforms than XBox or PlayStation. That's something significant even for big players in the industry. Simply speaking it's better to invest in 5-10 small projects than in single big project - it is likely that some of the small projects will perform below expectations, or even fail, but at the same time it is unlikely that the entire investment portfolio will be a loser because the risk will be highly diversified. A portfolio of big projects is more and more like playing poker with your entire bankroll on table ... Current state of THQ clearly shows what the effects can be - positive expected value is not enough to gamble with hundreds of milions of dollars (although some investment bankers thought differently :) ). Platform without support of developers is a dead platform, and developing for consoles in the classic model is not really attractive any more from business point of view. XBLA and PSN might still be a viable target platform, but as they are it's harder and harder to justify making any of them the lead platform - what we see more and more is that game gets launched on iOS/Android/Steam/Facebook and then possibly ported to consoles, not vice versa.